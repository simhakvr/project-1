{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9557 entries, 0 to 9556\n",
      "Columns: 132 entries, v2a1 to Target\n",
      "dtypes: float64(5), int64(124), object(3)\n",
      "memory usage: 9.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA - PYTHON\\lib\\site-packages\\ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\ANACONDA - PYTHON\\lib\\site-packages\\ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Score= 0.9973090148004186\n",
      "test_Score= 0.9170153417015342\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "hplp=pd.read_csv('C:/Users/91868/Desktop/Dataset_Poverty_Prediction.csv')\n",
    "hplp.shape\n",
    "\n",
    "#need to delete the columns that were square of the other columns\n",
    "#need to delete the non impacting columns in the given dataset i.e Id,idhogar\n",
    "hplp.drop(columns=['Id','idhogar','SQBescolari','SQBage','SQBhogar_total',\n",
    "                   'SQBedjefe','SQBhogar_nin','SQBovercrowding',\n",
    "                   'SQBdependency','SQBmeaned','agesq']\n",
    "                    ,inplace=True)\n",
    "\n",
    "hplp.info()\n",
    "hplp.isnull().sum()\n",
    "\n",
    "hplp.dtypes\n",
    "\n",
    "#Now, let's take a look at the target. We want to see the distribution of target values in the Dataset.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(hplp['Target'],bins=4)\n",
    "plt.title('Histogram Target counts')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Values')\n",
    "plt.show()\n",
    "\n",
    "#to check the unique Values in Target Column\n",
    "hplp['Target'].unique()\n",
    "#We saw that, there are only 4 numerical values in for the target. Value 4 seems to dominate\n",
    "\n",
    "#to fing the No.of Occurances of each value of Target \n",
    "pd.value_counts(hplp.Target)\n",
    "\n",
    "#filling nulll values before test, train splitting\n",
    "column=['v2a1','v18q1','rez_esc','meaneduc']\n",
    "for column in hplp[column]:\n",
    "    hplp[column].fillna(hplp[column].median(),inplace=True)\n",
    "    \n",
    "#Splitting the data into Train, Test Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y=hplp['Target']                  #assigning the output variable,i.e SalePrice to Y.\n",
    "hplp.drop(columns='Target',inplace=True)  #Dropping the particulat Output column \"SalePrice\"\n",
    "X_train,X_test,y_train,y_test=train_test_split(hplp,y,test_size=0.3,random_state=50)\n",
    "\n",
    "#need to de the Labelencode the Categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "catcols=[ele for ele in X_train.columns if X_train[ele].dtype==object]\n",
    "\n",
    "for col in catcols:\n",
    "    le=LabelEncoder()\n",
    "    #le.fit(X_train[col])\n",
    "    X_train[col]=le.fit_transform(X_train[col])\n",
    "    X_test[col]=le.transform(X_test[col])\n",
    "\n",
    "#Implementing the Gradientboost Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "                           learning_rate=0.2, loss='deviance', max_depth=6,\n",
    "                           max_features=None, max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                           min_samples_leaf=5, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                           n_iter_no_change=None, presort='auto',\n",
    "                           random_state=None, subsample=1.0, tol=0.0001,\n",
    "                           validation_fraction=0.1, verbose=0,\n",
    "                           warm_start=False)\n",
    "gbc.fit(X_train,y_train)\n",
    "\n",
    "print(\"train_Score=\",gbc.score(X_train,y_train))\n",
    "print(\"test_Score=\",gbc.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.2, loss='deviance', max_depth=6,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=5, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "0.8896696068171625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 5, 'max_depth': 6, 'learning_rate': 0.2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameter tuning for the above Gradient Boosting Classifier\n",
    "#to Supress the Warnings\n",
    "#Randomized search CV\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid1={'learning_rate':[0.1,0.2,0.3,0.4],'max_depth':[3,4,5,6],'min_samples_leaf':[4,5,3,2,6]}\n",
    "grid=RandomizedSearchCV(estimator=gbc,param_distributions=grid1,n_iter=5,cv=5)\n",
    "grid.fit(X_train,y_train)\n",
    "grid.get_params\n",
    "grid.cv_results_\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2653   10]\n",
      "  [  52  153]]\n",
      "\n",
      " [[2335   62]\n",
      "  [ 108  363]]\n",
      "\n",
      " [[2470   31]\n",
      "  [ 132  235]]\n",
      "\n",
      " [[ 823  220]\n",
      "  [  31 1794]]]\n",
      "[[ 153   21    5   26]\n",
      " [   5  363   16   87]\n",
      " [   4   21  235  107]\n",
      " [   1   20   10 1794]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.75      0.83       205\n",
      "           2       0.85      0.77      0.81       471\n",
      "           3       0.88      0.64      0.74       367\n",
      "           4       0.89      0.98      0.93      1825\n",
      "\n",
      "    accuracy                           0.89      2868\n",
      "   macro avg       0.89      0.79      0.83      2868\n",
      "weighted avg       0.89      0.89      0.88      2868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluation Metrics of the above Multiclass Classification \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "print(multilabel_confusion_matrix(y_test,gbc.predict(X_test)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test,gbc.predict(X_test)))\n",
    "\n",
    "print(classification_report(y_test,gbc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
      "[1 2 3 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9693781396660857"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AUC Score\n",
    "y_pred_proba=gbc.predict_proba(X_test)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "print(lb.fit(y_test))\n",
    "print(lb.classes_)\n",
    "y_test_new=lb.transform(y_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc=roc_auc_score(y_test_new,y_pred_proba)\n",
    "auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
